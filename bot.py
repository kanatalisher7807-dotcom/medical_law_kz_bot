import os
import json
import re
import logging
from difflib import SequenceMatcher
from typing import Optional, Dict, Any, List, Tuple

from aiogram import Bot, Dispatcher, types
from aiogram.utils import executor
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton

logging.basicConfig(level=logging.INFO)

TOKEN = os.getenv("TELEGRAM_TOKEN")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

DISCLAIMER = (
    "‚ö†Ô∏è –û—Ç–≤–µ—Ç –Ω–æ—Å–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä –∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º –∑–∞–∫–ª—é—á–µ–Ω–∏–µ–º. "
    "–î–ª—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫—É ¬´‚úâÔ∏è –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—é¬ª."
)

SECTIONS = [
    "‚öñÔ∏è –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –æ—à–∏–±–∫–∏",
    "üö® –ò–Ω—Ü–∏–¥–µ–Ω—Ç—ã",
    "üè• –ñ–∞–ª–æ–±—ã –ø–∞—Ü–∏–µ–Ω—Ç–∞",
    "‚úçÔ∏è –ò–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ",
    "üîí –í—Ä–∞—á–µ–±–Ω–∞—è —Ç–∞–π–Ω–∞",
    "üëÆ –û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –º–µ–¥—Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤",
    "üìÑ –ù–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞",
    "‚úâÔ∏è –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—é",
    "üß™ –ú–∏–Ω–∏-—Ç–µ—Å—Ç—ã",
]

# –ö–ª–∞–≤–∏–∞—Ç—É—Ä–∞ (–º–µ–Ω—é)
menu = ReplyKeyboardMarkup(resize_keyboard=True)
menu.add(KeyboardButton(SECTIONS[0]), KeyboardButton(SECTIONS[1]))
menu.add(KeyboardButton(SECTIONS[2]), KeyboardButton(SECTIONS[3]))
menu.add(KeyboardButton(SECTIONS[4]), KeyboardButton(SECTIONS[5]))
menu.add(KeyboardButton(SECTIONS[6]))
menu.add(KeyboardButton(SECTIONS[7]), KeyboardButton(SECTIONS[8]))

# –†–µ–∂–∏–º—ã (–µ—Å–ª–∏ –≤—Å—ë –∂–µ —Ö–æ—á–µ—à—å –æ—Å—Ç–∞–≤–ª—è—Ç—å "—ç–∫–∑–∞–º–µ–Ω-—Ä–µ–∂–∏–º" –∫–Ω–æ–ø–∫–æ–π)
USER_MODE: Dict[int, str] = {}


def resolve_path(env_var: str, filename: str) -> str:
    env_path = os.getenv(env_var)
    if env_path and os.path.exists(env_path):
        return env_path

    p1 = os.path.join(BASE_DIR, filename)
    if os.path.exists(p1):
        return p1

    p2 = os.path.join(os.getcwd(), filename)
    if os.path.exists(p2):
        return p2

    p3 = os.path.join(os.getcwd(), "medical_law_kz_bot", filename)
    if os.path.exists(p3):
        return p3

    return p1


FAQ_PATH = resolve_path("FAQ_PATH", "faq.json")
EXAM_PATH = resolve_path("EXAM_PATH", "exam.json")


def load_json_list(path: str, label: str) -> List[Dict[str, Any]]:
    try:
        logging.info(f"Loading {label} from: {path}")
        logging.info(f"{label} exists: {os.path.exists(path)}")
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if not isinstance(data, list):
            logging.warning(f"{label} is not a list. Using empty list.")
            return []
        logging.info(f"{label} loaded: {len(data)} entries")
        return data
    except Exception as e:
        logging.exception(f"Failed to load {label}: %s", e)
        return []


FAQ = load_json_list(FAQ_PATH, "FAQ")
EXAM = load_json_list(EXAM_PATH, "EXAM")


# ---------------------------
#  –®–∞–≥ 1: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + –æ–ø–µ—á–∞—Ç–∫–∏
# ---------------------------

ALIASES = {
    "–∂–ª–±–∞": "–∂–∞–ª–æ–±–∞",
    "–∂–∞–ª–æ–±": "–∂–∞–ª–æ–±–∞",
    "—Ö–∞–º–∏—Ç": "–≥—Ä—É–±–æ—Å—Ç—å",
    "—Ö–∞–º": "–≥—Ä—É–±–æ—Å—Ç—å",
    "–≤—Ä–∞—á —Ö–∞–º": "–≥—Ä—É–±–æ—Å—Ç—å",
    "–¥–∏–∞–≥–Ω": "–¥–∏–∞–≥–Ω–æ–∑",
    "–¥–∏–∞–≥": "–¥–∏–∞–≥–Ω–æ–∑",
    "–∫–æ–Ω—Ñ–ª": "–∫–æ–Ω—Ñ–ª–∏–∫—Ç",
    "–∫–æ–Ω—Ñ–ª–∏–∫—Ç —Å –≤—Ä–∞—á–æ–º": "–∫–æ–Ω—Ñ–ª–∏–∫—Ç",
    "–æ—Ç–∫–∞–∑": "–æ—Ç–∫–∞–∑–∞–ª–∏",
    "–Ω–µ –ø—Ä–∏–Ω—è–ª–∏": "–æ—Ç–∫–∞–∑–∞–ª–∏",
    "–Ω–µ –ø—Ä–∏–Ω—è–ª": "–æ—Ç–∫–∞–∑–∞–ª–∏",
    "—Ç–∞–π–Ω–∞": "–≤—Ä–∞—á–µ–±–Ω–∞—è —Ç–∞–π–Ω–∞",
    "—Å–æ–≥–ª–∞—Å–∏–µ": "–∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ",
    "–æ—à–∏–±–∫–∞": "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –æ—à–∏–±–∫–∞",
    "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å": "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –º–µ–¥—Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤",
}

STOP_WORDS = {"–∏", "–≤", "–≤–æ", "–Ω–∞", "–ø–æ", "–∑–∞", "–∫", "–∫–æ", "–æ", "–æ–±", "–æ—Ç", "—ç—Ç–æ", "—á—Ç–æ", "–∫–∞–∫", "–ª–∏"}


def clean_text(s: str) -> str:
    s = (s or "").strip().lower()
    # —É–±—Ä–∞—Ç—å —ç–º–æ–¥–∑–∏/—Å–ª—É–∂–µ–±–Ω–æ–µ: –æ—Å—Ç–∞–≤–ª—è–µ–º –±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã/–ø—Ä–æ–±–µ–ª
    s = re.sub(r"[^0-9a-z–∞-—è—ë\s-]+", " ", s, flags=re.IGNORECASE)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def normalize_query(s: str) -> str:
    s = clean_text(s)
    # –∞–ª–∏–∞—Å—ã –ø–æ –≤—Å–µ–π —Å—Ç—Ä–æ–∫–µ
    if s in ALIASES:
        s = ALIASES[s]
    return s


def tokens(s: str) -> List[str]:
    s = normalize_query(s)
    parts = [p for p in s.split() if p and p not in STOP_WORDS]
    return parts


def sim(a: str, b: str) -> float:
    return SequenceMatcher(None, a, b).ratio()


def best_match(entries: List[Dict[str, Any]], user_text: str, keyword_field: str = "keywords") -> Optional[Dict[str, Any]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à–∏–π entry –ø–æ:
    1) —Ç–æ—á–Ω—ã–º –≤—Ö–æ–∂–¥–µ–Ω–∏—è–º kw –≤ —Ç–µ–∫—Å—Ç
    2) fuzzy-—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º (–æ–ø–µ—á–∞—Ç–∫–∏/–æ–±—Ä–µ–∑–∫–∏)
    """
    text = normalize_query(user_text)
    tks = tokens(text)

    best: Optional[Dict[str, Any]] = None
    best_score: float = 0.0

    for e in entries:
        kws = e.get(keyword_field) or []
        if not isinstance(kws, list):
            continue

        score = 0.0

        for kw in kws:
            if not isinstance(kw, str):
                continue
            kw_n = normalize_query(kw)

            # 1) —Ç–æ—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã
            if kw_n and kw_n in text:
                score += 3.0
                continue

            # 2) fuzzy: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º kw —Å –∫–∞–∂–¥—ã–º —Ç–æ–∫–µ–Ω–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            for tk in tks:
                if not tk or not kw_n:
                    continue

                # –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–æ–∫–µ–Ω—ã —Ç–∏–ø–∞ "–¥–∏–∞–≥–Ω" ‚Äî —Ç–æ–∂–µ –ª–æ–≤–∏–º
                r = sim(tk, kw_n)
                if r >= 0.82:
                    score += 1.6
                elif len(tk) >= 4 and len(kw_n) >= 4 and (tk in kw_n or kw_n in tk):
                    score += 1.1

        # –±–æ–Ω—É—Å, –µ—Å–ª–∏ entry –≤ –Ω—É–∂–Ω–æ–π —Å–µ–∫—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if score > best_score:
            best_score = score
            best = e

    # –ø–æ—Ä–æ–≥: —á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏–ª –º—É—Å–æ—Ä
    return best if best_score >= 1.2 else None


def format_faq_answer(entry: Dict[str, Any]) -> str:
    answer = (entry.get("answer") or entry.get("a") or "").strip()
    law = (entry.get("law") or "").strip()
    if law:
        answer += f"\n\nüî∑ –ù–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞: {law}"
    answer += f"\n\n{DISCLAIMER}"
    return answer


def format_exam_answer(entry: Dict[str, Any]) -> str:
    q = (entry.get("question") or "").strip()
    ideal = (entry.get("ideal_answer") or "").strip()
    comment = (entry.get("comment") or "").strip()
    mistake = (entry.get("common_mistake") or "").strip()
    law = (entry.get("law") or "").strip()

    out = "üéì –≠–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω–∞—è –∫–∞—Ä—Ç–æ—á–∫–∞"
    if q:
        out += f"\n\nüìå –í–æ–ø—Ä–æ—Å:\n{q}"
    if ideal:
        out += f"\n\n‚úÖ –≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\n{ideal}"
    if comment:
        out += f"\n\nüí° –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:\n{comment}"
    if mistake:
        out += f"\n\n‚ö†Ô∏è –¢–∏–ø–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞:\n{mistake}"
    if law:
        out += f"\n\nüî∑ –ù–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞:\n{law}"

    out += f"\n\n{DISCLAIMER}"
    return out


# ---------------------------
#  Aiogram init
# ---------------------------

if not TOKEN:
    raise RuntimeError("TELEGRAM_TOKEN is not set.")

bot = Bot(token=TOKEN)
dp = Dispatcher(bot)


@dp.message_handler(commands=["start"])
async def start(message: types.Message):
    text = (
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø –±–æ—Ç –∫–∞—Ñ–µ–¥—Ä—ã –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –ø—Ä–∞–≤–∞.\n\n"
        "–Ø –ø–æ–º–æ–≥–∞—é —Å —Ç–∏–ø–æ–≤—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏: –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –æ—à–∏–±–∫–∏, –∏–Ω—Ü–∏–¥–µ–Ω—Ç—ã, –∂–∞–ª–æ–±—ã, "
        "–∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ, –≤—Ä–∞—á–µ–±–Ω–∞—è —Ç–∞–π–Ω–∞, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å.\n\n"
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª –∫–Ω–æ–ø–∫–∞–º–∏ –Ω–∏–∂–µ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º."
    )
    await message.answer(text, reply_markup=menu)


@dp.message_handler(commands=["help", "menu"])
async def help_cmd(message: types.Message):
    await message.answer("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª –∫–Ω–æ–ø–∫–∞–º–∏ –∏–ª–∏ –Ω–∞–ø–∏—à–∏—Ç–µ –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º.", reply_markup=menu)


# –•–∞—Ä–¥–∫–æ–¥: –ù–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞
@dp.message_handler(lambda m: (m.text or "").strip() == "üìÑ –ù–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞")
async def law_base(message: types.Message):
    text = (
        "üìÑ –ù–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞ (–æ—Ä–∏–µ–Ω—Ç–∏—Ä—ã):\n"
        "‚Ä¢ –ö–æ–¥–µ–∫—Å –†–ö ¬´–û –∑–¥–æ—Ä–æ–≤—å–µ –Ω–∞—Ä–æ–¥–∞ –∏ —Å–∏—Å—Ç–µ–º–µ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è¬ª\n"
        "‚Ä¢ –£–ö / –ö–æ–ê–ü / –ì–ö / –¢–ö –†–ö ‚Äî –ø–æ —Å–∏—Ç—É–∞—Ü–∏–∏\n"
        "‚Ä¢ –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã –º–µ–¥–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–∏–∫–∞–∑—ã —É–ø–æ–ª–Ω–æ–º–æ—á–µ–Ω–Ω–æ–≥–æ –æ—Ä–≥–∞–Ω–∞\n\n"
        "–ï—Å–ª–∏ –Ω–∞–ø–∏—à–µ—Ç–µ —Ç–µ–º—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´–≤—Ä–∞—á–µ–±–Ω–∞—è —Ç–∞–π–Ω–∞¬ª), —è –ø–æ–¥—Å–∫–∞–∂—É —Ç–∏–ø–æ–≤–æ–π –±–ª–æ–∫ –Ω–æ—Ä–º."
    )
    await message.answer(text, reply_markup=menu)


# –•–∞—Ä–¥–∫–æ–¥: –í–æ–ø—Ä–æ—Å –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—é
@dp.message_handler(lambda m: (m.text or "").strip() == "‚úâÔ∏è –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—é")
async def ask_teacher(message: types.Message):
    await message.answer(
        "–ù–∞–ø–∏—à–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å –æ–¥–Ω–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º.\n"
        "–§–æ—Ä–º–∞—Ç: *–¢–µ–º–∞* ‚Üí *–°—É—Ç—å –≤–æ–ø—Ä–æ—Å–∞*.\n"
        "–ù–µ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ –ª–∏—à–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.",
        parse_mode="Markdown",
        reply_markup=menu,
    )


# –ö–Ω–æ–ø–∫–∞: –ú–∏–Ω–∏-—Ç–µ—Å—Ç—ã (–æ—Å—Ç–∞–≤–∏–º —Ä–µ–∂–∏–º, –Ω–æ EXAM –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∏ –±–µ–∑ –Ω–µ–≥–æ)
@dp.message_handler(lambda m: (m.text or "").strip() == "üß™ –ú–∏–Ω–∏-—Ç–µ—Å—Ç—ã")
async def mini_tests(message: types.Message):
    USER_MODE[message.from_user.id] = "exam"
    await message.answer(
        "üß™ –≠–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º –≤–∫–ª—é—á—ë–Ω.\n\n"
        "–ü–∏—à–∏—Ç–µ —Ç–µ–º—É/–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞. –ß—Ç–æ–±—ã –≤—ã–π—Ç–∏ ‚Äî –Ω–∞–ø–∏—à–∏—Ç–µ: –≤—ã—Ö–æ–¥",
        reply_markup=menu,
    )


# –ö–Ω–æ–ø–∫–∏ —Ä–∞–∑–¥–µ–ª–æ–≤: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º intro, –µ—Å–ª–∏ –µ—Å—Ç—å (type="intro")
@dp.message_handler(lambda m: (m.text or "").strip() in SECTIONS)
async def handle_section_buttons(message: types.Message):
    key = (message.text or "").strip()

    intro = next((e for e in FAQ if e.get("section") == key and e.get("type") == "intro"), None)
    if intro:
        await message.answer(format_faq_answer(intro), reply_markup=menu)
        return

    # –µ—Å–ª–∏ intro –Ω–µ—Ç ‚Äî –ø—Ä–æ—Å—Ç–æ –ø–æ–∫–∞–∂–µ–º –±–∞–∑–æ–≤—É—é –ø–æ–¥—Å–∫–∞–∑–∫—É
    await message.answer(
        "–†–∞–∑–¥–µ–ª –æ—Ç–∫—Ä—ã—Ç. –ù–∞–ø–∏—à–∏—Ç–µ 1‚Äì2 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞ –ø–æ —Ç–µ–º–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–æ—Ç–∫–∞–∑–∞–ª–∏¬ª, ¬´–∂–∞–ª–æ–±–∞¬ª, ¬´–≤—Ä–∞—á–µ–±–Ω–∞—è —Ç–∞–π–Ω–∞¬ª).",
        reply_markup=menu
    )


# –ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (–≤–∞–∂–Ω–æ!)
@dp.message_handler(lambda m: m.text and (not m.text.startswith("/")) and ((m.text or "").strip() not in SECTIONS))
async def handle_text(message: types.Message):
    uid = message.from_user.id
    raw = (message.text or "").strip()

    # –≤—ã—Ö–æ–¥ –∏–∑ —ç–∫–∑–∞–º–µ–Ω-—Ä–µ–∂–∏–º–∞
    if USER_MODE.get(uid) == "exam" and raw.lower() in ("–≤—ã—Ö–æ–¥", "–≤—ã–π—Ç–∏", "exit"):
        USER_MODE.pop(uid, None)
        await message.answer("–≠–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º –≤—ã–∫–ª—é—á—ë–Ω. –ú–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –æ–±—ã—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã.", reply_markup=menu)
        return

    # 1) –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤ exam-—Ä–µ–∂–∏–º–µ ‚Äî —Å–Ω–∞—á–∞–ª–∞ EXAM
    if USER_MODE.get(uid) == "exam":
        exam_entry = best_match(EXAM, raw, keyword_field="keywords")
        if exam_entry:
            await message.answer(format_exam_answer(exam_entry), reply_markup=menu)
            return
        await message.answer(
            "–ü–æ —ç—Ç–æ–º—É –∑–∞–ø—Ä–æ—Å—É —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω–∞—è –∫–∞—Ä—Ç–æ—á–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–æ—â–µ: ¬´–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å¬ª, ¬´–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω–∞—è¬ª, ¬´—É–≥–æ–ª–æ–≤–Ω–∞—è¬ª.",
            reply_markup=menu,
        )
        return

    # 2) –æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º: —Å–Ω–∞—á–∞–ª–∞ FAQ (—Å fuzzy)
    faq_entry = best_match(FAQ, raw, keyword_field="keywords")
    if faq_entry:
        await message.answer(format_faq_answer(faq_entry), reply_markup=menu)
        return

    # 3) –µ—Å–ª–∏ –≤ FAQ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø—Ä–æ–±—É–µ–º EXAM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ (–®–∞–≥ 2)
    exam_entry = best_match(EXAM, raw, keyword_field="keywords")
    if exam_entry:
        await message.answer(format_exam_answer(exam_entry), reply_markup=menu)
        return

    # 4) —Å–æ–≤—Å–µ–º –Ω–∏—á–µ–≥–æ
    await message.answer(
        "–ù–µ –Ω–∞—à—ë–ª —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.\n"
        "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–æ—â–µ (1‚Äì2 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞) –∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ ¬´‚úâÔ∏è –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—é¬ª.",
        reply_markup=menu,
    )


if __name__ == "__main__":
    executor.start_polling(dp, skip_updates=True)
